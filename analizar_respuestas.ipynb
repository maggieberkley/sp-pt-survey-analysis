{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALIZAR RESPUESTAS**"
      ],
      "metadata": {
        "id": "fB96rkrS9Guz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Autenticación\n",
        "Este paso de autenticación es necesario para que Colab pueda acceder al Sheet que tiene las respuestas. Se abrirán dos ventanas y tendrás que aceptar los permisos para continuar."
      ],
      "metadata": {
        "id": "KHoaMWVW8R0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conecta tu unidad de Google Drive.\n",
        "# Se abrirá otra ventana. Aceptá los permisos para continuar.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RCbxNfVwTG0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autentica al usuario en Google Colab y configura las credenciales para acceder a Google Sheets.\n",
        "# Se abrirá otra ventana. Aceptá los permisos para continuar.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "9HuHiEdP9oLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Abre y lee el Sheet que tiene las respuestas para analizar.\n",
        "# Este Sheet debe estar en tu unidad de Drive y debe tener una sola columna con una respuesta por fila.\n",
        "\n",
        "# El nombre del Sheet debe ser \"Respuestas\" (o si querés ponerle otro nombre, asegurate de cambiar el nombre en la siguiente línea de código).\n",
        "# Debe tener dos hojas, \"respuestas_es\" con las respuestas en español y \"respuestas_pt\" con las respuestas en portugués.\n",
        "\n",
        "sheet = gc.open(\"Respuestas\")\n",
        "\n",
        "# Lee las respuestas en español.\n",
        "f1 = sheet.get_worksheet(0)\n",
        "respuestas_es = f1.get_all_values()\n",
        "\n",
        "# Lee las respuestas en portugués.\n",
        "f2 = sheet.get_worksheet(1)\n",
        "respuestas_pt = f2.get_all_values()"
      ],
      "metadata": {
        "id": "uWYfl32L-GU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime la lista de las respuestas.\n",
        "print(respuestas_es)\n",
        "print(respuestas_pt)"
      ],
      "metadata": {
        "id": "OlO8cLj1_DRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clasificación de texto\n",
        "Esta parte del código utiliza el clasificador para categorizar las respuestas en español por tema (producto, servicio, general). Este clasificador no funciona con respuestas en portugués."
      ],
      "metadata": {
        "id": "DECD-kBZqtTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "gBWoun5CTK-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiYhP4r_RsoA"
      },
      "outputs": [],
      "source": [
        "# Carga el clasificador entrenado en las respuestas a las encuestas anteriores.\n",
        "# Asegurate de que el clasificador y el tokenizador no estén en ninguna carpeta en Drive.\n",
        "# Si preferís que permanezcan en una carpeta, asegurate de cambiar las rutas en las siguientes líneas de código.\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/model_sinprecio2\")\n",
        "\n",
        "# Carga el tokenizador pre-entrenado.\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/model_sinprecio2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un diccionario para convertir los IDs a etiquetas.\n",
        "# El modelo que entrené le asignó el 0 a \"general\", el 1 a \"producto\" y el 2 a \"servicio\".\n",
        "id2label = {0: \"general\", 1: \"producto\", 2: \"servicio\"}\n",
        "\n",
        "# Hace una lista para guardar los resultados del análisis de las respuestas en español.\n",
        "resultados_es = []"
      ],
      "metadata": {
        "id": "ecX2PQfuFEN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En esta sección, el clasificador hace predicciones.\n",
        "\n",
        "# Para cada respuesta...\n",
        "for respuesta in respuestas_es:\n",
        "    inputs = loaded_tokenizer(respuesta[0], return_tensors=\"pt\")\n",
        "    outputs = loaded_model(**inputs)\n",
        "    # Hace una predicción.\n",
        "    clase = outputs.logits.argmax(dim=-1).item()\n",
        "    # Agrega la predicción a la lista de resultados.\n",
        "    resultados_es.append([respuesta[0], id2label[clase]])"
      ],
      "metadata": {
        "id": "0uU9wMRG8Jxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime la lista de resultados actualizada.\n",
        "\n",
        "# Ahora tenemos una lista de las respuestas en español y su categoría.\n",
        "print(resultados_es)"
      ],
      "metadata": {
        "id": "owNNZXlaCktX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Análisis de sentimiento\n",
        "Esta parte del código utiliza dos modelos de análisis de sentimiento entrenados con tweets en español y portugués ([pysentimiento](https://github.com/pysentimiento/pysentimiento/)) para analizar el sentimiento de las respuestas en ambos idiomas."
      ],
      "metadata": {
        "id": "uDUXwu18qoGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Este paso puede tardar un poco.\n",
        "!pip install pysentimiento transformers accelerate"
      ],
      "metadata": {
        "id": "TA_fJMAXG0-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentimientos = {\"NEG\": \"negativo\", \"POS\": \"positivo\", \"NEU\": \"neutro\"}"
      ],
      "metadata": {
        "id": "4JuGr_Csc1h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pysentimiento import create_analyzer\n",
        "\n",
        "# Crea un analyzer con la tarea de análisis de sentimiento en español.\n",
        "analyzer_es = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Hace predicciones de sentimiento y las guarda en la lista de resultados.\n",
        "for i in range(len(resultados_es)):\n",
        "    sentimiento = analyzer_es.predict(resultados_es[i][0])\n",
        "    resultados_es[i].append(sentimientos[sentimiento.output])"
      ],
      "metadata": {
        "id": "oyf5d6pGG5WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hace una lista para guardar los resultados del análisis de las respuestas en portugués.\n",
        "resultados_pt = [x for x in respuestas_pt]\n",
        "\n",
        "# Crea un analyzer con la tarea de análisis de sentimiento en portugués.\n",
        "analyzer_pt = create_analyzer(task=\"sentiment\", lang=\"pt\")\n",
        "\n",
        "# Hace predicciones de sentimiento y las guarda en la lista de resultados.\n",
        "for i in range(len(resultados_pt)):\n",
        "    sentimiento = analyzer_pt.predict(resultados_pt[i][0])\n",
        "    resultados_pt[i].append(sentimientos[sentimiento.output])"
      ],
      "metadata": {
        "id": "Bd6o3GZnYxdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime las listas de resultados actualizadas.\n",
        "\n",
        "# Ahora tenemos una lista con las respuestas en español, su categoría y su sentimiento.\n",
        "print(resultados_es)\n",
        "\n",
        "# Y tenemos otra lista con las respuestas en portugués y su sentimiento.\n",
        "print(resultados_pt)"
      ],
      "metadata": {
        "id": "QzIbFXACL9zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extracción de palabras clave\n",
        "\n",
        "Esta parte del programa extrae los sustantivos más importantes de cada respuesta utilizando [este paquete de spaCy](https://spacy.io/models/es) para las respuestas en español y [este paquete](https://spacy.io/models/pt) para las respuestas en portugués.\n",
        "\n",
        "Para hacerlo, usé como referencia [este artículo sobre el modelado temático con texto en español](https://medium.com/@jarch/topic-modeling-with-spanish-text-an-introductory-example-in-python-9cdb10fbe126), [este artículo sobre el análisis de texto](https://medium.com/nlplanet/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06#3363) y [este artículo sobre spaCy y gensim](https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf).\n",
        "\n"
      ],
      "metadata": {
        "id": "fQvW41vZqyFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga el paquete de \"es_core_news_sm\" de spaCy.\n",
        "# Este paso puede tardar un poco.\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "3DSbN5TdWQkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "import spacy\n",
        "\n",
        "# Descarga una lista de stopwords, o sea, palabras funcionales y palabras frecuentes en español (ej. \"de\", \"los\", \"que\").\n",
        "# Estas palabras las vamos a eliminar para obtener una lista de palabras clave.\n",
        "nltk.download(\"stopwords\")\n",
        "spanish_stopwords = set(nltk_stopwords.words(\"spanish\"))\n",
        "\n",
        "# Agrega unos stopwords personalizados, propios de este proyecto.\n",
        "# Por ejemplo, muchas respuestas mencionan Loyal Solutions pero no queremos que se agreguen esas palabras a la lista de palabras clave.\n",
        "es_custom_stopwords = [\"loyal\", \"solutions\", \"vez\", \"veces\", \"cosa\", \"año\", \"usamos\"]\n",
        "\n",
        "for w in es_custom_stopwords:\n",
        "    spanish_stopwords.add(w)\n",
        "\n",
        "print(spanish_stopwords)"
      ],
      "metadata": {
        "id": "Sr5oPheSWTj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga el paquete de NLP en español.\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Do something to make sure words don't repeat? FIXMEEE\n",
        "\n",
        "# Define una función para normalizar, tokenizar y lematizar las respuestas en español.\n",
        "# Esta función recibe una respuesta, limpia la respuesta y genera una lista de palabras clave.\n",
        "def es_normalize_tokenize_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    VALID_POS = {'PROPN', 'NOUN'}\n",
        "    keywords = []\n",
        "    for word in doc:\n",
        "        if word.lemma_ and word.lemma_.lower() not in spanish_stopwords and word.lemma_.lower() not in keywords:\n",
        "            if word.pos_ in VALID_POS:\n",
        "                keywords.append(word.lemma_.lower())\n",
        "    return keywords\n",
        "\n",
        "# Actualiza la lista de resultados en español.\n",
        "for resultado in resultados_es:\n",
        "    resultado.append(\", \".join(es_normalize_tokenize_lemmatize(resultado[0])))"
      ],
      "metadata": {
        "id": "glKcUFrN5-JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga el paquete de \"pt_core_news_sm\" de spaCy.\n",
        "# Este paso puede tardar un poco.\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "eDGHLC6hWfXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora, repetimos el proceso de arriba con las respuestas en portugués.\n",
        "\n",
        "# Descarga una lista de stopwords, o sea, palabras funcionales y palabras frecuentes en portugués (ej. \"de\", \"os\", \"que\").\n",
        "# Estas palabras las vamos a eliminar para obtener una lista de palabras clave.\n",
        "portuguese_stopwords = set(nltk_stopwords.words(\"portuguese\"))\n",
        "\n",
        "# Agrega unos stopwords personalizados, propios de este proyecto.\n",
        "# Por ejemplo, muchas respuestas mencionan las palabras Loyal y Solutions pero no queremos que aparezcan a la lista de palabras clave.\n",
        "pt_custom_stopwords = [\"loyal\", \"solutions\", \"vez\", \"vezes\"]\n",
        "\n",
        "for w in pt_custom_stopwords:\n",
        "    portuguese_stopwords.add(w)\n",
        "\n",
        "print(portuguese_stopwords)"
      ],
      "metadata": {
        "id": "3-IUqEctWf72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga el paquete de NLP en portugués.\n",
        "nlp2 = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Define una función para normalizar, tokenizar y lematizar las respuestas en portugués.\n",
        "# Esta función recibe una respuesta, limpia la respuesta y genera una lista de palabras clave.\n",
        "def pt_normalize_tokenize_lemmatize(text):\n",
        "    doc = nlp2(text)\n",
        "    VALID_POS = {'PROPN', 'NOUN'}\n",
        "    keywords = []\n",
        "    for word in doc:\n",
        "        if word.lemma_ and word.lemma_.lower() not in portuguese_stopwords and word.lemma_.lower() not in keywords:\n",
        "            if word.pos_ in VALID_POS:\n",
        "                keywords.append(word.lemma_.lower())\n",
        "    return keywords\n",
        "\n",
        "# Actualiza la lista de resultados en portugués.\n",
        "for resultado in resultados_pt:\n",
        "    resultado.append(\", \".join(pt_normalize_tokenize_lemmatize(resultado[0])))"
      ],
      "metadata": {
        "id": "t-YO1xoD7aPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime los resultados finales.\n",
        "print(resultados_es)\n",
        "print(resultados_pt)"
      ],
      "metadata": {
        "id": "WFPWC1g3A0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Combinar y analizar"
      ],
      "metadata": {
        "id": "ctBpo_p1ZYsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicia una lista de todos los resultados.\n",
        "resultados_todo = [[\"Respuesta\", \"Idioma\", \"Categoría\", \"Sentimiento\", \"Palabras clave\"]]\n",
        "\n",
        "# Combina los resultados de las respuestas en español y los de las respuestas en portugués.\n",
        "for i in range(len(resultados_es)):\n",
        "    resultados_todo.append([resultados_es[i][0], \"español\", resultados_es[i][1], resultados_es[i][2], resultados_es[i][3]])\n",
        "for i in range(len(resultados_pt)):\n",
        "    resultados_todo.append([resultados_pt[i][0], \"portugués\", \"\", resultados_pt[i][1], resultados_pt[i][2]])"
      ],
      "metadata": {
        "id": "2dLxajZocQuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convierte la lista de todos los resultados en Dataframe.\n",
        "resultados_df = pd.DataFrame(resultados_todo)"
      ],
      "metadata": {
        "id": "MGPydqsvR7QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza un análisis básico de todos los resultados.\n",
        "\n",
        "analisis = [[\"Categoría\", \"Cantidad de respuestas\"]]\n",
        "\n",
        "num_es = resultados_df[resultados_df[1] == \"español\"].shape[0]\n",
        "num_pt = resultados_df[resultados_df[1] == \"portugués\"].shape[0]\n",
        "analisis.append([\"Respuestas en español\", num_es])\n",
        "analisis.append([\"Respuestas en portugués\", num_pt])\n",
        "\n",
        "# Categoría\n",
        "num_general = resultados_df[resultados_df[2] == \"general\"].shape[0]\n",
        "num_producto = resultados_df[resultados_df[2] == \"producto\"].shape[0]\n",
        "num_servicio = resultados_df[resultados_df[2] == \"servicio\"].shape[0]\n",
        "analisis.append([\"Respuestas sobre el producto\", num_producto])\n",
        "analisis.append([\"Respuestas sobre el servicio\", num_servicio])\n",
        "analisis.append([\"Respuestas generales\", num_general])\n",
        "\n",
        "# Sentimiento\n",
        "num_negativo = resultados_df[resultados_df[3] == \"negativo\"].shape[0]\n",
        "num_positivo = resultados_df[resultados_df[3] == \"positivo\"].shape[0]\n",
        "num_neutro = resultados_df[resultados_df[3] == \"neutro\"].shape[0]\n",
        "analisis.append([\"Respuestas positivas\", num_positivo])\n",
        "analisis.append([\"Respuestas negativas\", num_negativo])\n",
        "analisis.append([\"Respuestas neutras\", num_neutro])\n",
        "\n",
        "# Categoría y sentimiento\n",
        "pos_prod = resultados_df[resultados_df[2] == \"producto\"][resultados_df[3] == \"positivo\"].shape[0]\n",
        "neg_prod = resultados_df[resultados_df[2] == \"producto\"][resultados_df[3] == \"negativo\"].shape[0]\n",
        "neu_prod = resultados_df[resultados_df[2] == \"producto\"][resultados_df[3] == \"neutro\"].shape[0]\n",
        "pos_serv = resultados_df[resultados_df[2] == \"servicio\"][resultados_df[3] == \"positivo\"].shape[0]\n",
        "neg_serv = resultados_df[resultados_df[2] == \"servicio\"][resultados_df[3] == \"negativo\"].shape[0]\n",
        "neu_serv = resultados_df[resultados_df[2] == \"servicio\"][resultados_df[3] == \"neutro\"].shape[0]\n",
        "pos_gen = resultados_df[resultados_df[2] == \"general\"][resultados_df[3] == \"positivo\"].shape[0]\n",
        "neg_gen = resultados_df[resultados_df[2] == \"general\"][resultados_df[3] == \"negativo\"].shape[0]\n",
        "neu_gen = resultados_df[resultados_df[2] == \"general\"][resultados_df[3] == \"neutro\"].shape[0]\n",
        "\n",
        "analisis.append([\"Respuestas positivas sobre el producto\", pos_prod])\n",
        "analisis.append([\"Respuestas positivas sobre el servicio\", pos_serv])\n",
        "analisis.append([\"Respuestas positivas generales\", pos_gen])\n",
        "\n",
        "analisis.append([\"Respuestas negativas sobre el producto\", neg_prod])\n",
        "analisis.append([\"Respuestas negativas sobre el servicio\", neg_serv])\n",
        "analisis.append([\"Respuestas negativas generales\", neg_gen])\n",
        "\n",
        "analisis.append([\"Respuestas neutras sobre el producto\", neu_prod])\n",
        "analisis.append([\"Respuestas neutras sobre el servicio\", neu_serv])\n",
        "analisis.append([\"Respuestas neutras generales\", neu_gen])"
      ],
      "metadata": {
        "id": "wNtvVzzw-93H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(analisis)"
      ],
      "metadata": {
        "id": "11Bz4VoFCwW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierte la lista de análisis en Dataframe.\n",
        "analisis_df = pd.DataFrame(analisis[1:], columns=analisis[0])\n",
        "analisis_df.head(20)"
      ],
      "metadata": {
        "id": "suyvDB6qBsIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras clave\n",
        "from collections import Counter\n",
        "\n",
        "palabras_clave = []\n",
        "for i in range(len(resultados_todo)):\n",
        "    palabras_clave += resultados_todo[i][4].split(\", \")\n",
        "palabras_dict = Counter(palabras_clave)\n",
        "\n",
        "print(palabras_dict)\n",
        "\n",
        "palabras = [[\"Palabra\", \"Frecuencia\"]]\n",
        "\n",
        "for k, v in palabras_dict.items():\n",
        "    if k != \"\" and v > 3:\n",
        "        palabras.append([k, v])\n",
        "\n",
        "print(palabras)"
      ],
      "metadata": {
        "id": "7Zqk_LVhFUV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convierte el diccionario de palabras clave en Dataframe.\n",
        "palabras_df = pd.DataFrame(palabras[1:], columns=palabras[0])\n",
        "palabras_df.head(20)"
      ],
      "metadata": {
        "id": "rvcyoJbvNzE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Exportar"
      ],
      "metadata": {
        "id": "9KzEeTPUeQLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea cinco nuevas hojas del Sheet llamadas \"resultados_es\", \"resultados_pt\", \"resultados_todo\", \"análisis\" y \"palabras_clave\"\n",
        "# y escribe los datos en el Sheet.\n",
        "spreadsheet = gc.open(\"Respuestas\")\n",
        "\n",
        "# resultados_es tiene los resultados de las respuestas en español.\n",
        "sheet_es = spreadsheet.add_worksheet(title=\"resultados_es\", rows=str(len(resultados_es)+2), cols=str(len(resultados_es[0])+2))\n",
        "sheet_es.update(\"A1\", [[\"Respuesta\", \"Categoría\", \"Sentimiento\", \"Palabras clave\"]] + resultados_es)\n",
        "\n",
        "# resultados_pt tiene los resultados de las respuestas en portugués.\n",
        "sheet_pt = spreadsheet.add_worksheet(title=\"resultados_pt\", rows=str(len(resultados_pt)+2), cols=str(len(resultados_pt[0])+2))\n",
        "sheet_pt.update(\"A1\", [[\"Respuesta\", \"Sentimiento\", \"Palabras clave\"]] + resultados_pt)\n",
        "\n",
        "# resultados_todo tiene los resultados de todas las respuestas en español y portugués.\n",
        "sheet_todo = spreadsheet.add_worksheet(title=\"resultados_todo\", rows=str(len(resultados_todo)+2), cols=str(len(resultados_todo[0])+2))\n",
        "sheet_todo.update(\"A1\", resultados_todo)\n",
        "\n",
        "# análisis tiene las cantidades de respuestas en diferentes categorías.\n",
        "sheet_analisis = spreadsheet.add_worksheet(title=\"análisis\", rows=str(len(analisis)+2), cols=str(len(analisis[0])+2))\n",
        "sheet_analisis.update(\"A1\", analisis)\n",
        "\n",
        "# palabras_clave tiene una lista de las palabras clave y sus frecuencias correspondientes.\n",
        "sheet_palabras = spreadsheet.add_worksheet(title=\"palabras_clave\", rows=str(len(palabras)+2), cols=str(len(palabras[0])+2))\n",
        "sheet_palabras.update(\"A1\", palabras)"
      ],
      "metadata": {
        "id": "cWZwlLYrxPsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}